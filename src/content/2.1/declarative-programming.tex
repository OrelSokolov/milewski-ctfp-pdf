% !TEX root = ../../ctfp-print.tex

\lettrine[lhang=0.17]{В}{первой части} книги я утверждал, что и теория категорий, и
программирование о композиционности. В программировании вы продолжаете
разлагать проблему, пока не достигнете уровня детализации, с которым вы можете
справиться, решите каждую подпроблему по очереди и рекомпозируете решения
снизу вверх. Есть, грубо говоря, два способа сделать это: сказав
компьютеру, что делать, или сказав ему, как это делать. Один называется
декларативным, а другой императивным.

Вы можете увидеть это даже на самом базовом уровне. Сама композиция может быть
определена декларативно; как в, \code{h} --- это композит \code{g}
после \code{f}:

\src{snippet01}
или императивно; как в, вызвать \code{f} сначала, запомнить результат
этого вызова, затем вызвать \code{g} с результатом:

\src{snippet02}
Императивная версия программы обычно описывается как последовательность
действий, упорядоченных во времени. В частности, вызов \code{g} не может
произойти до завершения выполнения \code{f}. По крайней мере, такова
концептуальная картина --- в ленивом языке с передачей аргументов
\emph{по необходимости}, фактическое выполнение может протекать по-другому.

На самом деле, в зависимости от сообразительности компилятора, может быть
мало или вообще нет разницы между тем, как выполняется декларативный и императивный код.
Но две методологии различаются, иногда резко, в
способе, которым мы подходим к решению проблем, и в поддерживаемости и
тестируемости результирующего кода.

Главный вопрос: когда мы сталкиваемся с проблемой, всегда ли у нас есть
выбор между декларативным и императивным подходами к её решению?
И, если есть декларативное решение, может ли оно всегда быть переведено
в компьютерный код? Ответ на этот вопрос далёк от очевидного, и,
если бы мы могли его найти, мы, вероятно, революционизировали бы наше понимание
вселенной.

\begin{wrapfigure}{R}{0pt}
  \includegraphics[width=0.5\textwidth]{images/asteroids.png}
\end{wrapfigure}

Позвольте мне разработать. В физике есть похожая двойственность, которая либо
указывает на некоторый глубокий лежащий в основе принцип, либо говорит нам что-то о
том, как работают наши умы. Ричард Фейнман упоминает эту двойственность как
вдохновение в своей собственной работе по квантовой электродинамике.

Есть две формы выражения большинства законов физики. Одна использует локальные,
или инфинитезимальные, рассмотрения. Мы смотрим на состояние системы
в малой окрестности и предсказываем, как она будет эволюционировать в течение
следующего мгновения времени. Это обычно выражается с использованием дифференциальных
уравнений, которые должны быть проинтегрированы, или просуммированы, за период
времени.

Обратите внимание, как этот подход напоминает императивное мышление: мы достигаем
финального решения, следуя последовательности малых шагов, каждый из которых зависит от
результата предыдущего. На самом деле, компьютерные симуляции
физических систем обычно реализуются превращением дифференциальных
уравнений в разностные уравнения и их итерированием. Так
анимируются космические корабли в игре asteroids. На каждом временном шаге
позиция космического корабля изменяется добавлением малого приращения, которое
вычисляется умножением его скорости на временную дельту. Скорость,
в свою очередь, изменяется малым приращением, пропорциональным ускорению,
которое задаётся силой, делённой на массу.

Это прямые кодировки дифференциальных уравнений,
соответствующих законам движения Ньютона:
\begin{align*}
  F & = m \frac{dv}{dt} \\
  v & = \frac{dx}{dt}
\end{align*}
Похожие методы могут быть применены к более сложным проблемам, таким как
распространение электромагнитных полей с использованием уравнений Максвелла, или даже
поведение кварков и глюонов внутри протона с использованием решёточной \acronym{QCD}
(квантовой хромодинамики).

Это локальное мышление в сочетании с дискретизацией пространства и времени,
которое поощряется использованием цифровых компьютеров, нашло своё крайнее
выражение в героической попытке Стивена Вольфрама свести
сложность всей вселенной к системе клеточных автоматов.

Другой подход глобальный. Мы смотрим на начальное и конечное состояние
системы и вычисляем траекторию, которая их соединяет,
минимизируя определённый функционал. Простейший пример --- принцип
наименьшего времени Ферма. Он утверждает, что световые лучи распространяются вдоль путей,
которые минимизируют их время полёта. В частности, в отсутствие
отражающих или преломляющих объектов световой луч из точки $A$ в точку $B$
возьмёт кратчайший путь, который является прямой линией. Но свет
распространяется медленнее в плотных (прозрачных) материалах, таких как вода или стекло.
Так что если вы выберете начальную точку в воздухе, а конечную точку под
водой, для света более выгодно путешествовать дольше в воздухе, а
затем взять кратчайший путь через воду. Путь минимального времени заставляет
луч преломиться на границе воздуха и воды, приводя к закону преломления
Снелла:
\begin{equation*}
  \frac{sin(\theta_1)}{sin(\theta_2)} = \frac{v_1}{v_2}
\end{equation*}
где $v_1$ --- скорость света в воздухе, а $v_2$ ---
скорость света в воде.

\begin{figure}[H]
  \centering
  \includegraphics[width=0.3\textwidth]{images/snell.jpg}
\end{figure}

\noindent
Вся классическая механика может быть выведена из принципа наименьшего
действия. Действие может быть вычислено для любой траектории интегрированием
лагранжиана, который является разностью между кинетической и потенциальной
энергией (заметьте: это разность, не сумма --- сумма была бы
полной энергией). Когда вы стреляете из миномёта, чтобы поразить данную цель,
снаряд сначала пойдёт вверх, где потенциальная энергия из-за гравитации
выше, и проведёт там некоторое время, накапливая отрицательный вклад в
действие. Он также замедлится на вершине параболы, чтобы
минимизировать кинетическую энергию. Затем он ускорится, чтобы быстро пройти через
область низкой потенциальной энергии.

\begin{figure}[H]
  \centering
  \includegraphics[width=0.35\textwidth]{images/mortar.jpg}
\end{figure}

\noindent
Величайший вклад Фейнмана состоял в том, чтобы понять, что принцип
наименьшего действия может быть обобщён на квантовую механику. Там снова
проблема формулируется в терминах начального состояния и конечного состояния.
Интеграл по путям Фейнмана между этими состояниями используется для вычисления
вероятности перехода.

\begin{figure}[H]
  \centering
  \includegraphics[width=0.35\textwidth]{images/feynman.jpg}
\end{figure}

\noindent
Суть в том, что есть любопытная необъяснённая двойственность в способе, которым мы
можем описывать законы физики. Мы можем использовать локальную картину, в которой
вещи происходят последовательно и малыми приращениями. Или мы можем использовать
глобальную картину, где мы объявляем начальные и конечные условия, а
всё между ними просто следует.

Глобальный подход также может быть использован в программировании, например, при
реализации трассировки лучей. Мы объявляем позицию глаза и
позиции источников света и выясняем пути, которые световые лучи
могут взять, чтобы соединить их. Мы не минимизируем явно время
полёта для каждого луча, но мы используем закон Снелла и геометрию
отражения с тем же эффектом.

Самая большая разница между локальным и глобальным подходом --- в
их трактовке пространства и, что более важно, времени. Локальный подход
принимает немедленное удовлетворение здесь и сейчас, тогда как глобальный
подход принимает долгосрочный статический взгляд, как если бы будущее было
предопределено, и мы только анализировали свойства некоторой вечной
вселенной.

Нигде это не иллюстрируется лучше, чем в подходе функционального реактивного
программирования (\acronym{FRP}) к взаимодействию с пользователем. Вместо написания отдельных
обработчиков для каждого возможного действия пользователя, все имеющие доступ к некоторому
общему изменяемому состоянию, \acronym{FRP} рассматривает внешние события как бесконечный список
и применяет серию преобразований к нему. Концептуально список
всех наших будущих действий там, доступен как входные данные для нашей
программы. С перспективы программы нет разницы между
списком цифр $\pi$, списком псевдослучайных чисел или списком позиций мыши,
приходящих через компьютерное оборудование. В каждом случае, если вы хотите
получить $n^\text{й}$ элемент, вы должны сначала пройти через первые $n-1$ элементов. Когда
применяется к временным событиям, мы называем это свойство \emph{причинностью}.

Так какое это имеет отношение к теории категорий? Я буду утверждать, что
теория категорий поощряет глобальный подход и, следовательно, поддерживает
декларативное программирование. Прежде всего, в отличие от математического анализа, она не имеет
встроенного понятия расстояния, или окрестности, или времени. Всё, что у нас есть, ---
это абстрактные объекты и абстрактные связи между ними. Если вы можете попасть
из $A$ в $B$ через серию шагов, вы также можете попасть туда одним
прыжком. Более того, главный инструмент теории категорий --- универсальная
конструкция, которая является воплощением глобального подхода. Мы видели её
в действии, например, в определении категорного произведения.
Оно было сделано спецификацией его свойств --- очень декларативный
подход. Это объект, оснащённый двумя проекциями, и это
лучший такой объект --- он оптимизирует определённое свойство: свойство
факторизации проекций других таких объектов.

\begin{figure}[H]
  \centering
  \includegraphics[width=0.35\textwidth]{images/productranking.jpg}
\end{figure}

\noindent
Сравните это с принципом минимального времени Ферма или принципом
наименьшего действия.

И наоборот, сравните это с традиционным определением декартова
произведения, которое гораздо более императивно. Вы описываете, как создать
элемент произведения, выбрав один элемент из одного множества и другой
элемент из другого множества. Это рецепт создания пары. И есть
другой для разборки пары.

Почти в каждом языке программирования, включая функциональные языки,
такие как Haskell, типы произведения, типы копроизведения и типы функций
встроены, а не определяются универсальными конструкциями; хотя
были попытки создания категорных языков программирования
(см., например,
\urlref{http://web.sfc.keio.ac.jp/~hagino/thesis.pdf}{диссертацию Татсуи
  Хагино}).

Используются ли напрямую или нет, категорные определения обосновывают
существующие программные конструкции и порождают новые. Что наиболее
важно, теория категорий предоставляет мета-язык для рассуждений
о компьютерных программах на декларативном уровне. Она также поощряет
рассуждения о спецификации проблемы до того, как она будет отлита в код.
